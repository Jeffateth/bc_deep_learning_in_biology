{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "\n",
      "=== unfreeze_last_n = 0 ===\n",
      "Module                     Params    Trainable\n",
      "----------------------------------------------\n",
      "esm                     7,512,474        8,960\n",
      "head                      330,753      330,753\n",
      "----------------------------------------------\n",
      "Trainable                 339,713\n",
      "Non-trainable           7,503,514\n",
      "Total                   7,843,227\n",
      "Size (MB)                 29.920\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8941f3062f4a20a6d91dde4d6ec188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/6:   0%|          | 0/21692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 264\u001b[39m\n\u001b[32m    260\u001b[39m cfg_run[\u001b[33m\"\u001b[39m\u001b[33munfreeze_last_n\u001b[39m\u001b[33m\"\u001b[39m] = n\n\u001b[32m    262\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== unfreeze_last_n = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m model, alphabet, history, best_path, best_rmse, save_dir = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_run\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m    265\u001b[39m plot_history(history, save_dir=save_dir)                                    \n\u001b[32m    266\u001b[39m plot_predictions(model, alphabet, cfg_run, save_dir=save_dir)               \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(cfg)\u001b[39m\n\u001b[32m    145\u001b[39m wt, mut, y = wt.to(DEVICE), mut.to(DEVICE), y.to(DEVICE) \u001b[38;5;66;03m# y is the true ΔΔG values for the batch.\u001b[39;00m\n\u001b[32m    146\u001b[39m loss = nn.functional.mse_loss(model(wt, mut), y) \u001b[38;5;66;03m# calculates the MSE over batch (by default uses mean)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m opt.zero_grad(); \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m; torch.nn.utils.clip_grad_norm_([p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad], cfg.get(\u001b[33m\"\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1.0\u001b[39m)); opt.step() \u001b[38;5;66;03m# \"zero_grad()\" clears old gradients, \"backward\" computes gradients via autograd; gradient clipping is a technique used to prevent exploding gradients by scaling down the gradients if their norm exceeds a specified threshold (grad_clip); this can help stabilize training, especially when fine-tuning large models like ESM, by ensuring that the updates to the model's parameters do not become excessively large, which can lead to divergence or unstable training dynamics.\u001b[39;00m\n\u001b[32m    148\u001b[39m bs = y.numel() \u001b[38;5;66;03m# number of samples in the batch; multiplying the loss (which is an average over the batch) by the batch size gives the total squared error for that batch, which we accumulate in train_sse to compute the overall training RMSE at the end of the epoch.\u001b[39;00m\n\u001b[32m    149\u001b[39m train_sse += loss.item() * bs \u001b[38;5;66;03m# accumulates the total squared error for the batch into train_sse, which will be used to calculate the training RMSE at the end of the epoch; multiplying the loss (which is the mean squared error for the batch) by the number of samples in the batch (bs) gives the total squared error for that batch, which we sum across all batches to get the total squared error for the entire training set in that epoch.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/DL in Biology/Dry_Lab/bc_deep_learning_in_biology/venv/lib/python3.13/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/DL in Biology/Dry_Lab/bc_deep_learning_in_biology/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/Universität/ETH/DL in Biology/Dry_Lab/bc_deep_learning_in_biology/venv/lib/python3.13/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ── Imports ───────────────────────────────────────────────────────────────────\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import esm\n",
    "from datetime import datetime\n",
    "\n",
    "# ── Setup ─────────────────────────────────────────────────────────────────────\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "random.seed(42); np.random.seed(42); torch.manual_seed(42)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "CFG = dict(\n",
    "    train_csv  = \"project_data/mega_train.csv\",\n",
    "    val_csv    = \"project_data/mega_val.csv\",\n",
    "    label_col  = \"ddG_ML\",\n",
    "    epochs     = 6,\n",
    "    batch_size = 10,\n",
    "    lr_head    = 1e-3,\n",
    "    lr_esm     = 5e-6,\n",
    "    patience   = 5, # number of epochs to wait for improvement in validation RMSE before early stopping\n",
    "    grad_clip  = 1.0, # maximum allowed norm of the gradients during training; this helps prevent exploding gradients by scaling down the gradients if their norm exceeds this threshold.\n",
    "    lr_factor  = 0.5, # factor by which to reduce the learning rates when validation RMSE plateaus; for example, if lr_factor=0.5, then the learning rates will be halved when the scheduler triggers a learning rate reduction due to lack of improvement in validation RMSE.\n",
    "    lr_patience= 2, # number of epochs with no improvement in validation RMSE after which to reduce the learning rates by the specified factor; for example, if lr_patience=2, then if the validation RMSE does not improve for 2 consecutive epochs, the learning rates will be reduced by multiplying them with lr_factor (e.g., halved if lr_factor=0.5).\n",
    ")\n",
    "\n",
    "# ── Dataset ───────────────────────────────────────────────────────────────────\n",
    "class ProteinPairDataset(Dataset):\n",
    "    def __init__(self, csv_path, label_col=\"ddG_ML\"):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        self.wt  = df[\"wt_seq\"].tolist()\n",
    "        self.mut = df[\"aa_seq\"].tolist()\n",
    "        self.y   = df[label_col].astype(float).values\n",
    "        self.ids = df[\"name\"].tolist()\n",
    "\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.wt[i], self.mut[i], float(self.y[i]), self.ids[i]\n",
    "\n",
    "\n",
    "def make_collate(batch_converter): # tokenize sequences into ESM input\n",
    "    def collate(batch):\n",
    "        wt, mut, y, ids = zip(*batch) # unzips the batch\n",
    "        _, _, wt_tok  = batch_converter(list(zip(ids, wt))) # ESM models take tokenized tensors\n",
    "        _, _, mut_tok = batch_converter(list(zip(ids, mut))) # mut_tok: tokenized mutant batch [B, L] (batch, sequence length)\n",
    "        return wt_tok, mut_tok, torch.tensor(y, dtype=torch.float32), list(ids)\n",
    "    return collate\n",
    "\n",
    "# ── Model ─────────────────────────────────────────────────────────────────────\n",
    "    \n",
    "class DDGPredictor(nn.Module):\n",
    "    def __init__(self, esm_model, alphabet, hidden=256, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.esm, self.alphabet = esm_model, alphabet\n",
    "        self.repr_layer = esm_model.num_layers # extract representations from the last layer\n",
    "        d = esm_model.embed_dim # d: per-token embedding dimension; for esm2_t6_8M_UR50D, d=320\n",
    "        self.head = nn.Sequential( # simple MLP head that takes \"engineered paired features\" (concatenated pooled representations) to map to ΔΔG\n",
    "            nn.LayerNorm(4*d), # Input diemnsions: (B,4*d). Normalizes the input features across the last dimension, because the four ESM representations can have different scales (like m*w is different than m-w) and LayerNorm can help stabilize training.\n",
    "            nn.Linear(4*d, hidden), nn.GELU(), nn.Dropout(dropout), # fully connected layer, learning a weighted combination of the concatenated features, with GELU non-linear activation function and dropout (randomly zero some hidden neurons during training with the probability 0.2) for regularization\n",
    "            nn.Linear(hidden, 1), # final linear layer that maps the hidden representation to a single scalar output (the predicted ΔΔG)\n",
    "        )\n",
    "\n",
    "    def encode(self, tokens): # encode a batch of tokenized sequences into pooled representations by averaging the per-token embeddings (ignoring padding and EOS tokens)\n",
    "        h = self.esm(tokens, repr_layers=[self.repr_layer], return_contacts=False) # h: dictionary with keys \"representations\" and \"contacts\"; we only care about the representations, which is a list of length num_layers, where each element is a tensor of shape [B, L, d] (batch size, sequence length, embedding dimension)\n",
    "        h = h[\"representations\"][self.repr_layer] # h: tensor of shape [B, L, d] containing the per-token embeddings from the last layer of the ESM model\n",
    "        mask = (tokens != self.alphabet.padding_idx) & (tokens != self.alphabet.eos_idx) # Sequences vary strongly in length. Short sequences were padded to the length of the longest on ein the batch, because matrix multiplication requires fixed shapes. mask: boolean tensor of shape [B, L] where True indicates valid tokens (not padding or EOS) and False indicates invalid tokens; this is used to mask out the padding and EOS tokens when averaging the embeddings.\n",
    "        mask[:, 0] = False # also ignore the CLS token at the beginning of the sequence, which is not a real amino acid and can have a different embedding distribution than the other tokens. (: -> all rows, 0 -> first column)\n",
    "        return (h * mask.unsqueeze(-1).float()).sum(1) / mask.sum(1, keepdim=True).float() # mask.unsqueeze(-1) changes shape from [B, L] to [B, L, 1],expanding the mask to apply across all \"d\" embedding dimensions;  \".float\" converts boolean mask to 0/1 for multiplication;  \"h * mask\" : All \"masked = false\" tokens get automatically embedding = 0; \".sum(1)\" summing all token embeddings within each protein, collapsing [B, L, d] to [B, d], which is the average embedding for each sequence in the batch, where the average is taken over the valid tokens (ignoring padding, EOS, and CLS)\n",
    "\n",
    "    def forward(self, wt, mut): # \n",
    "        w, m = self.encode(wt), self.encode(mut) # each sequence is passed independently through the shared ESM encoder to get their pooled representations w and m, each of shape [B, d] --> Siamese architecture: the same ESM encoder processes both the wild-type and mutant sequences, allowing it to learn a shared representation space for both types of inputs, which can help the model learn how mutations affect the protein's properties by comparing their embeddings.\n",
    "        return self.head(torch.cat([w, m, m-w, m*w], dim=-1)).squeeze(-1) # passes the concatenated features [w, m, m-w, m*w] of shape [B, 4*d] through the regression head, and outputs a single scalar prediction for each sequence pair; the features include the individual embeddings of the wild-type and mutant sequences (w and m), as well as their element-wise difference (m-w) and product (m*w), which can capture different aspects of how the mutation affects the protein's properties; .squeeze(-1) removes the last dimension of size 1 from the output, resulting in a tensor of shape [B] containing the predicted ΔΔG values for each sequence pair in the batch.\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    trainable     = sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "    non_trainable = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    print(f\"{'Module':<20} {'Params':>12} {'Trainable':>12}\")\n",
    "    print(\"-\" * 46)\n",
    "    for name, module in model.named_children():\n",
    "        p  = sum(x.numel() for x in module.parameters()) # total number of parameters in the module, calculated by summing the number of elements in each parameter tensor; for example, if the ESM model has 8 million parameters and the head has 0.5 million parameters, then p for the ESM module would be 8 million and p for the head module would be 0.5 million.\n",
    "        pt = sum(x.numel() for x in module.parameters() if x.requires_grad) # number of trainable parameters in the module, which is a subset of the total parameters; this is calculated by summing the number of elements in each parameter tensor that has requires_grad=True, indicating that it will be updated during training; for example, if the ESM model's parameters are frozen (requires_grad=False), then pt for the ESM module would be 0, while for the head module, pt would equal p since all its parameters are trainable.\n",
    "        print(f\"{name:<20} {p:>12,} {pt:>12,}\")\n",
    "    print(\"-\" * 46)\n",
    "    print(f\"{'Trainable':<20} {trainable:>12,}\")\n",
    "    print(f\"{'Non-trainable':<20} {non_trainable:>12,}\")\n",
    "    print(f\"{'Total':<20} {trainable+non_trainable:>12,}\")\n",
    "    print(f\"{'Size (MB)':<20} {(trainable+non_trainable)*4/1024**2:>11.3f}\")\n",
    "\n",
    "# ── Train ─────────────────────────────────────────────────────────────────────\n",
    "def train(cfg):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = f\"checkpoints_esm_finetune_{timestamp}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    best_path = os.path.join(save_dir, f\"ddg_best_{timestamp}.pt\")\n",
    "\n",
    "    esm_model, alphabet = esm.pretrained.esm2_t6_8M_UR50D() # loads the pretrained ESM-2 model with 6 layers and 8 million parameters, trained on the UR50D dataset; esm_model is the actual PyTorch model that can be used for encoding sequences, while alphabet is an object that contains the vocabulary and tokenization logic for converting amino acid sequences into token IDs that the model can process.\n",
    "\n",
    "    for p in esm_model.parameters(): p.requires_grad = False # freezes the ESM model's parameters by setting requires_grad to False, which means that during training, the gradients for these parameters will not be computed and they will not be updated by the optimizer;\n",
    "    n = int(cfg.get(\"unfreeze_last_n\", 0)); [p.requires_grad_(True) for b in (esm_model.layers[-n:] if n > 0 else []) for p in b.parameters()] # unfreezes the parameters of the last layer of the ESM model, allowing them to be fine-tuned during training;\n",
    "    for m in esm_model.modules(): # Layernorms standardize hidden features within each layer using scaling and shifting, and unfreezing them allows the model to adapt its normalization to the new task, prevents overfitting.\n",
    "        if isinstance(m, nn.LayerNorm): # unfreezes all LayerNorm parameters in the ESM model, which can help stabilize training and allow the model to adapt its normalization to the new task, even if the main weights of the ESM model are mostly frozen;\n",
    "            for p in m.parameters(): p.requires_grad = True\n",
    "\n",
    "    model   = DDGPredictor(esm_model, alphabet).to(DEVICE) # initializes the DDGPredictor model, which consists of the ESM encoder and a regression head, and moves it to the specified device (GPU or CPU) for training; the model will use the pretrained ESM encoder to extract features from the input sequences, and the regression head will learn to map those features to the ΔΔG predictions based on the training data.\n",
    "    collate = make_collate(alphabet.get_batch_converter()) # creates a collate function for the DataLoader using the batch converter from the alphabet, which will be used to tokenize the input sequences on-the-fly during training and validation; this allows the DataLoader to take raw amino acid sequences from the dataset and convert them into the tokenized format required by the ESM model when forming batches.\n",
    "\n",
    "    train_dl = DataLoader(ProteinPairDataset(cfg[\"train_csv\"]), cfg[\"batch_size\"], shuffle=True,  collate_fn=collate) # initializes the DataLoader for the training dataset, which will load data from the specified CSV file, create batches of the specified size, shuffle the data at the beginning of each epoch to improve training, and use the custom collate function to tokenize the sequences; this DataLoader will yield batches of tokenized wild-type and mutant sequences along with their corresponding ΔΔG labels during training.\n",
    "    val_dl   = DataLoader(ProteinPairDataset(cfg[\"val_csv\"]),   cfg[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "    opt = torch.optim.AdamW([ # Adam optimizer adjusts the effective gradient update step size applied to each parameter during backpropagation, but does not reduce the global learning rate over time.\n",
    "        {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and not n.startswith(\"esm.\")], \"lr\": cfg[\"lr_head\"]},\n",
    "        {\"params\": [p for n,p in model.named_parameters() if p.requires_grad and     n.startswith(\"esm.\")], \"lr\": cfg[\"lr_esm\"]},\n",
    "    ], weight_decay=1e-2) # L2 regularization weight decay penalizes large weights and reduces overfitting\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( # scheduler that reduces the global learning rates alpha when the validation RMSE plateaus, which can help the model converge to a better solution by allowing it to take smaller steps when it is no longer improving; it monitors the validation RMSE and if it does not improve for a certain number of epochs (lr_patience), it multiplies the learning rates by a specified factor (lr_factor) to reduce them.\n",
    "        opt,\n",
    "        mode=\"min\",\n",
    "        factor=cfg.get(\"lr_factor\", 0.5),\n",
    "        patience=cfg.get(\"lr_patience\", 2),\n",
    "    )\n",
    "\n",
    "    count_params(model)\n",
    "\n",
    "    history = {\"train_rmse\": [], \"val_rmse\": [], \"pearson\": [], \"spearman\": []}\n",
    "\n",
    "    # Initialize best state safely\n",
    "    best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "    best_rmse = float(\"inf\")\n",
    "    patience_left = cfg[\"patience\"]\n",
    "\n",
    "    for epoch in range(1, cfg[\"epochs\"] + 1): # start counting epochs from 1 for better readability in logs and plots\n",
    "        # train\n",
    "        model.train()\n",
    "        model.esm.train(cfg.get(\"unfreeze_last_n\", 0) > 0) # set the esm backbone to training mode only if we unfreeze any layer to finetune.\n",
    "        train_sse = 0.0 # train_sse (sum of squared errors) accumulates the total squared error across all training samples in the current epoch, which is used to calculate the training RMSE at the end of the epoch by dividing it by the total number of samples (train_n) and taking the square root; this metric gives an indication of how well the model is fitting the training data, with lower values indicating better fit.\n",
    "        train_n = 0 # train_n counts the total number of training samples processed in the current epoch, which is used to calculate the training RMSE by dividing the accumulated sum of squared errors (train_sse) by this count and taking the square root; this ensures that the RMSE is correctly normalized by the number of samples, giving an average error per sample.\n",
    "        pbar = tqdm(train_dl, desc=f\"Epoch {epoch}/{cfg['epochs']}\", leave=False)\n",
    "        for wt, mut, y, _ in pbar:\n",
    "            wt, mut, y = wt.to(DEVICE), mut.to(DEVICE), y.to(DEVICE) # y is the true ΔΔG values for the batch.\n",
    "            loss = nn.functional.mse_loss(model(wt, mut), y) # calculates the MSE over batch (by default uses mean)\n",
    "            opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_([p for p in model.parameters() if p.requires_grad], cfg.get(\"grad_clip\", 1.0)); opt.step() # \"zero_grad()\" clears old gradients, \"backward\" computes gradients via autograd; gradient clipping is a technique used to prevent exploding gradients by scaling down the gradients if their norm exceeds a specified threshold (grad_clip); this can help stabilize training, especially when fine-tuning large models like ESM, by ensuring that the updates to the model's parameters do not become excessively large, which can lead to divergence or unstable training dynamics.\n",
    "            bs = y.numel() # number of samples in the batch; multiplying the loss (which is an average over the batch) by the batch size gives the total squared error for that batch, which we accumulate in train_sse to compute the overall training RMSE at the end of the epoch.\n",
    "            train_sse += loss.item() * bs # accumulates the total squared error for the batch into train_sse, which will be used to calculate the training RMSE at the end of the epoch; multiplying the loss (which is the mean squared error for the batch) by the number of samples in the batch (bs) gives the total squared error for that batch, which we sum across all batches to get the total squared error for the entire training set in that epoch.\n",
    "            train_n += bs # accumulates the total number of training samples processed in the epoch by adding the batch size (bs) for each batch; this count is used to calculate the training RMSE at the end of the epoch by dividing the accumulated sum of squared errors (train_sse) by this count and taking the square root, giving an average error per sample.\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"}) # update the progress bar \n",
    "\n",
    "        # validate\n",
    "        model.eval() \n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad(): # disables gradient calculation during validation\n",
    "            for wt, mut, y, _ in val_dl: # iterates over the validation DataLoader\n",
    "                preds.append(model(wt.to(DEVICE), mut.to(DEVICE)).cpu().numpy()) # for each batch, the model's predictions are computed and appended to the preds list, while the true ΔΔG values (y) are appended to the trues list; after processing all validation batches, these lists will contain the predictions and true values for the entire validation set, which can then be concatenated into single arrays for calculating the validation RMSE and correlation metrics.\n",
    "                trues.append(y.cpu().numpy())\n",
    "\n",
    "        p, t       = np.concatenate(preds), np.concatenate(trues) # concatenates the predictions and true values from all validation batches into single arrays p and t, which are then used to calculate the validation RMSE and correlation metrics; this allows us to evaluate the model's performance on the entire validation set after processing it in batches.\n",
    "        train_rmse = math.sqrt(train_sse / train_n) # by dividing the accumulated sum of squared errors (train_sse) by the total number of training samples (train_n);\n",
    "        val_rmse   = float(np.sqrt(np.mean((p - t) ** 2))) # calculates the validation RMSE by taking the square root of the mean squared error between the predicted values (p) and the true values (t) for the entire validation set;\n",
    "        r          = float(pearsonr(p, t)[0]) # calculates the Pearson correlation coefficient (r) between the predicted values (p) and the true values (t) for the validation set, which measures the linear correlation between the predictions and the true values.\n",
    "        rho        = float(spearmanr(p, t)[0]) # calculates the Spearman rank correlation coefficient (ρ) between the predicted values (p) and the true values (t) for the validation set, which measures the monotonic relationship between the predictions and the true values, regardless of whether that relationship is linear or not.\n",
    "        scheduler.step(val_rmse) # updates the learning rates according to the ReduceLROnPlateau scheduler based on the validation RMSE; if the validation RMSE does not improve for a certain number of epochs (lr_patience), the scheduler will reduce the learning rates by multiplying them with a specified factor (lr_factor), which can help the model converge to a better solution by allowing it to take smaller steps when it is no longer improving.\n",
    "        lr_head = opt.param_groups[0][\"lr\"] # retrieves the current learning rate for the head parameters from the optimizer's parameter groups for logging.\n",
    "        lr_esm  = opt.param_groups[1][\"lr\"]\n",
    "\n",
    "        history[\"train_rmse\"].append(train_rmse)\n",
    "        history[\"val_rmse\"].append(val_rmse)\n",
    "        history[\"pearson\"].append(r)\n",
    "        history[\"spearman\"].append(rho)\n",
    "\n",
    "        if epoch % 1 == 0: # Save history after every epoch\n",
    "            history_path = os.path.join(save_dir, f\"history_epoch_{epoch}.pth\")\n",
    "            torch.save(history, history_path)\n",
    "            print(f\"  → Saved history to: {history_path}\")\n",
    "\n",
    "        print(f\"Epoch {epoch:2d} | Train RMSE: {train_rmse:.4f} | Val RMSE: {val_rmse:.4f} | r: {r:.3f} | ρ: {rho:.3f}| lr_head: {lr_head:.2e} | lr_esm: {lr_esm:.2e}\")\n",
    "\n",
    "        if val_rmse < best_rmse - 1e-4:\n",
    "            best_rmse, patience_left = val_rmse, cfg[\"patience\"] # update the best RMSE and reset the patience\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "            checkpoint = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_rmse\": best_rmse,\n",
    "                \"cfg\": cfg,\n",
    "                \"model_state\": best_state,\n",
    "                \"optimizer_state\": opt.state_dict(),\n",
    "                \"scheduler_state\": scheduler.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"  → Saved best checkpoint to: {best_path}\")\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "            if patience_left == 0:\n",
    "                print(f\"Early stopping. Best RMSE: {best_rmse:.4f}\"); break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, alphabet, history, best_path, best_rmse\n",
    "\n",
    "# ── Plot ──────────────────────────────────────────────────────────────────────\n",
    "def plot_history(history):\n",
    "    epochs = range(1, len(history[\"train_rmse\"]) + 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
    "\n",
    "    axes[0].plot(epochs, history[\"train_rmse\"], label=\"Train\")\n",
    "    axes[0].plot(epochs, history[\"val_rmse\"],   label=\"Val\")\n",
    "    axes[0].set(xlabel=\"Epoch\", ylabel=\"RMSE (kcal/mol)\", title=\"Learning Curves\")\n",
    "    axes[0].legend(frameon=False)\n",
    "\n",
    "    axes[1].plot(epochs, history[\"pearson\"],  label=\"Pearson r\")\n",
    "    axes[1].plot(epochs, history[\"spearman\"], label=\"Spearman ρ\")\n",
    "    axes[1].set(xlabel=\"Epoch\", ylabel=\"Correlation\", title=\"Validation Correlations\")\n",
    "    axes[1].legend(frameon=False)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_history.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions(model, alphabet, cfg):\n",
    "    collate = make_collate(alphabet.get_batch_converter())\n",
    "    val_dl  = DataLoader(ProteinPairDataset(cfg[\"val_csv\"]), cfg[\"batch_size\"], collate_fn=collate)\n",
    "    preds, trues = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for wt, mut, y, _ in val_dl:\n",
    "            preds.append(model(wt.to(DEVICE), mut.to(DEVICE)).cpu().numpy())\n",
    "            trues.append(y.cpu().numpy())\n",
    "    p, t = np.concatenate(preds), np.concatenate(trues)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "    ax.hexbin(p, t, gridsize=50, cmap=\"Blues\", mincnt=1)\n",
    "    lim = [min(p.min(), t.min()), max(p.max(), t.max())]\n",
    "    ax.plot(lim, lim, \"k--\", lw=1, alpha=0.5)\n",
    "    ax.set(xlabel=\"Predicted ΔΔG\", ylabel=\"Measured ΔΔG\", aspect=\"equal\")\n",
    "    ax.text(0.05, 0.95,\n",
    "            f\"RMSE = {np.sqrt(np.mean((p-t)**2)):.3f}\\nr = {pearsonr(p,t)[0]:.3f}\",\n",
    "            transform=ax.transAxes, va=\"top\",\n",
    "            bbox=dict(boxstyle=\"round\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    ax.spines[\"top\"].set_visible(False); ax.spines[\"right\"].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"predictions.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# ── Run ───────────────────────────────────────────────────────────────────────\n",
    "sweep = [0, 1, 2, 3]\n",
    "results = []\n",
    "all_histories = {}\n",
    "\n",
    "for n in sweep:\n",
    "    cfg_run = dict(CFG)\n",
    "    cfg_run[\"unfreeze_last_n\"] = n\n",
    "\n",
    "    print(f\"\\n=== unfreeze_last_n = {n} ===\")\n",
    "\n",
    "    model, alphabet, history, best_path, best_rmse, save_dir = train(cfg_run)  \n",
    "    plot_history(history, save_dir=save_dir)                                    \n",
    "    plot_predictions(model, alphabet, cfg_run, save_dir=save_dir)               \n",
    "    final_history_path = os.path.join(save_dir, f\"final_history_unfreeze_{n}.pth\") \n",
    "    torch.save(history, final_history_path)\n",
    "    results.append((n, best_rmse, best_path))\n",
    "    all_histories[n] = history\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for n, rmse, path in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"n={n} | best_rmse={rmse:.4f} | {path}\")\n",
    "\n",
    "# Sweep comparison plot — saved next to the best run's folder, or just in cwd\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "for n, h in all_histories.items():\n",
    "    ax.plot(range(1, len(h[\"val_rmse\"]) + 1), h[\"val_rmse\"], label=f\"unfreeze={n}\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Val RMSE\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Validation RMSE Comparison Across Unfreeze Levels\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sweep_comparison.pdf\", bbox_inches=\"tight\")  # top-level; spans all runs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
