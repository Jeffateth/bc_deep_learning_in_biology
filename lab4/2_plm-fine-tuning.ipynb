{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning protein language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Describe the problem of **predicting the subcellular location** of (prokaryotic) proteins as described in [Moreno et al., 2024](https://doi.org/10.1093/bioinformatics/btae677)? Think of a biological question one could answer with proteome-wide predictions of subcellular location, and potential follow-up experiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, sklearn.preprocessing\n",
    "import datasets, evaluate, transformers # Hugging Face libraries https://doi.org/10.18653/v1/2020.emnlp-demos.6\n",
    "import Bio.SeqIO.FastaIO # Biopython for reading fasta files\n",
    "from IPython.display import display, HTML\n",
    "random_number = 4 # https://xkcd.com/221/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 5492k  100 5492k    0     0  10.6M      0 --:--:-- --:--:-- --:--:-- 10.6M\n"
     ]
    }
   ],
   "source": [
    "# Uncomment & execute once to download data from https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/\n",
    "!mkdir -p data\n",
    "!curl https://services.healthtech.dtu.dk/services/DeepLocPro-1.0/data/graphpart_set.fasta -o data/graphpart_set.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Look at the contents of `df_data`, how was the column `fold_id` defined in the paper? What exact set of sequences are in this data set (check number of rows)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`fold_id`**: 5-fold **homology-partition** label created with **GraphPart**; sequences in different folds have **≤30% Needleman–Wunsch identity** and folds are **balanced** for organism group + location (64 sequences removed to achieve separation).\n",
    "- **Dataset sequences**: experimentally verified prokaryotic proteins (UniProt release 2023_03 + PSORTdb 4.0), deduplicated, **len ≥ 40 aa**, **single-label only**; after GraphPart the training dataset has **11,906 sequences** (**= number of rows in `df_data`**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11890</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "8560      Q8A0Z3          Cytoplasmic       negative        0   \n",
       "8568      Q8A2N8          Cytoplasmic       negative        0   \n",
       "2593      P32709  CYtoplasmicMembrane       negative        0   \n",
       "61        E7FHF8          Cytoplasmic        archaea        0   \n",
       "63        E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "5239      Q97F85          Cytoplasmic       positive        4   \n",
       "5226      P33656          Cytoplasmic       positive        4   \n",
       "11894     P13949        OuterMembrane       negative        4   \n",
       "11890     P42185        Extracellular       negative        4   \n",
       "11902     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  \n",
       "8560   MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...  \n",
       "8568   MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...  \n",
       "2593   MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...  \n",
       "61     MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...  \n",
       "63     MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...  \n",
       "...                                                  ...  \n",
       "5239   MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...  \n",
       "5226   MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...  \n",
       "11894  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...  \n",
       "11890  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...  \n",
       "11902  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...  \n",
       "\n",
       "[11906 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/graphpart_set.fasta') as handle:\n",
    "    fasta_cols = ['header', 'sequence']\n",
    "    df_data = pd.DataFrame.from_records([values for values in Bio.SeqIO.FastaIO.SimpleFastaParser(handle)], columns=fasta_cols)\n",
    "header_cols = ['uniprot_id', 'subcellular_location', 'organism_group', 'fold_id']\n",
    "df_data[header_cols] = df_data['header'].str.split('|', expand=True)\n",
    "final_cols = ['uniprot_id']\n",
    "df_data = df_data[['uniprot_id', 'subcellular_location', 'organism_group', 'fold_id', 'sequence']].astype({'fold_id': int}).sort_values('fold_id')\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your 6 labels are: <ArrowStringArray>\n",
      "[        'Cytoplasmic', 'CYtoplasmicMembrane',            'Cellwall',\n",
      "       'OuterMembrane',       'Extracellular',         'Periplasmic']\n",
      "Length: 6, dtype: str\n",
      "\n",
      "Counts per location:\n",
      "subcellular_location\n",
      "Cytoplasmic            6885\n",
      "CYtoplasmicMembrane    2535\n",
      "Extracellular          1077\n",
      "OuterMembrane           756\n",
      "Periplasmic             566\n",
      "Cellwall                 87\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# List the 6 unique subcellular locations\n",
    "unique_locations = df_data['subcellular_location'].unique()\n",
    "print(f\"Your 6 labels are: {unique_locations}\")\n",
    "\n",
    "# See the distribution (how many proteins per location)\n",
    "print(\"\\nCounts per location:\")\n",
    "print(df_data['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>subcellular_location</th>\n",
       "      <th>organism_group</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>Q8A0Z3</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>Q8A2N8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2593</th>\n",
       "      <td>P32709</td>\n",
       "      <td>CYtoplasmicMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>E7FHF8</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>E7FHU4</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>archaea</td>\n",
       "      <td>0</td>\n",
       "      <td>MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>Q97F85</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>P33656</td>\n",
       "      <td>Cytoplasmic</td>\n",
       "      <td>positive</td>\n",
       "      <td>4</td>\n",
       "      <td>MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11894</th>\n",
       "      <td>P13949</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11890</th>\n",
       "      <td>P42185</td>\n",
       "      <td>Extracellular</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>Q0P986</td>\n",
       "      <td>OuterMembrane</td>\n",
       "      <td>negative</td>\n",
       "      <td>4</td>\n",
       "      <td>MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11906 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      uniprot_id subcellular_location organism_group  fold_id  \\\n",
       "8560      Q8A0Z3          Cytoplasmic       negative        0   \n",
       "8568      Q8A2N8          Cytoplasmic       negative        0   \n",
       "2593      P32709  CYtoplasmicMembrane       negative        0   \n",
       "61        E7FHF8          Cytoplasmic        archaea        0   \n",
       "63        E7FHU4          Cytoplasmic        archaea        0   \n",
       "...          ...                  ...            ...      ...   \n",
       "5239      Q97F85          Cytoplasmic       positive        4   \n",
       "5226      P33656          Cytoplasmic       positive        4   \n",
       "11894     P13949        OuterMembrane       negative        4   \n",
       "11890     P42185        Extracellular       negative        4   \n",
       "11902     Q0P986        OuterMembrane       negative        4   \n",
       "\n",
       "                                                sequence  label  \n",
       "8560   MAVTMADITKLRKMTGAGMMDCKNALTEAEGDYDKAMEIIRKKGQA...      2  \n",
       "8568   MIMSKETLIKSIREIPDFPIPGILFYDVTTLFKDPWCLQELSNIMF...      2  \n",
       "2593   MTQTSAFHFESLVWDWPIAIYLFLIGISAGLVTLAVLLRRFYPQAG...      0  \n",
       "61     MKLGVFELTDCGGCALNLLFLYDKLLDLLEFYEIAEFHMATSKKSR...      2  \n",
       "63     MGKVRIGFYALTSCYGCQLQLAMMDELLQLIPNAEIVCWFMIDRDS...      2  \n",
       "...                                                  ...    ...  \n",
       "5239   MRKLFTSESVTEGHPDKICDQISDAILDAILEKDPNGRVACETTVT...      2  \n",
       "5226   MKNKTEVKNGGEKKNSKKVSKEESAKEKNEKMKIVKNLIDKGKKSG...      2  \n",
       "11894  MCALDRRERPLNSQSVNKYILNVQNIYRNSPVPVCVRNKNRKILYA...      4  \n",
       "11890  MRLRFSVPLFFFGCVFVHGVFAGPFPPPGMSLPEYWGEEHVWWDGR...      3  \n",
       "11902  MKTRFSLILSACLLSSSLFAKNTDDEITKLQKQLAQIQAELAQIRK...      4  \n",
       "\n",
       "[11906 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode subcellular location as numerical labels\n",
    "subcellular_location_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "subcellular_location_encoder.fit(df_data['subcellular_location'])\n",
    "df_data['label'] = subcellular_location_encoder.transform(df_data['subcellular_location'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** How were the data partitioned during training and evaluation in the paper? What does the code below do, and how does it compare to the approach taken in the paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Paper partitioning:** 5-fold **GraphPart** homology split (≤30% identity across folds) + **nested CV**.  \n",
    "  Outer loop: 1 fold = **test**, remaining 4 folds used for **train/val** in **4 inner-loop** combinations ⇒ **20 models** total; test predictions per fold are **averaged over the 4 inner models**.\n",
    "\n",
    "- **Your code:** a **single fixed split** of folds:  \n",
    "  train = {0,1,2} (7336), eval = {3} (2579), test = {4} (1991); then prints class counts.\n",
    "\n",
    "- **Difference:** your approach is **not nested CV** and does **not rotate** the test fold or average across inner models; it’s a **one-shot hold-out** split (still homology-separated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7336 records in training data:\n",
      "subcellular_location\n",
      "Cytoplasmic            4345\n",
      "CYtoplasmicMembrane    1534\n",
      "Extracellular           645\n",
      "OuterMembrane           433\n",
      "Periplasmic             320\n",
      "Cellwall                 59\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2579 records in eval data:\n",
      "subcellular_location\n",
      "Cytoplasmic            1568\n",
      "CYtoplasmicMembrane     476\n",
      "Extracellular           224\n",
      "OuterMembrane           159\n",
      "Periplasmic             136\n",
      "Cellwall                 16\n",
      "Name: count, dtype: int64\n",
      "\n",
      "1991 records in test data:\n",
      "subcellular_location\n",
      "Cytoplasmic            972\n",
      "CYtoplasmicMembrane    525\n",
      "Extracellular          208\n",
      "OuterMembrane          164\n",
      "Periplasmic            110\n",
      "Cellwall                12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_id = {0, 1, 2}\n",
    "eval_id = {3}\n",
    "test_id = {4}\n",
    "\n",
    "df_train = df_data.query('fold_id in @train_id')#.groupby('subcellular_location').sample(n=50, random_state=random_number)\n",
    "df_eval = df_data.query('fold_id in @eval_id')\n",
    "df_test = df_data.query('fold_id in @test_id')\n",
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print()\n",
    "print(len(df_test), 'records in test data:')\n",
    "print(df_test['subcellular_location'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train), 'records in training data:')\n",
    "print(df_train['subcellular_location'].value_counts())\n",
    "print(df_train['organism_group'].value_counts())\n",
    "print()\n",
    "print(len(df_eval), 'records in eval data:')\n",
    "print(df_eval['subcellular_location'].value_counts())\n",
    "print(df_eval['organism_group'].value_counts())\n",
    "print()\n",
    "print(len(df_test), 'records in test data:')\n",
    "print(df_test['subcellular_location'].value_counts())\n",
    "print(df_test['organism_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/eval/test data sets for ESM2 model\n",
    "model_checkpoint = 'facebook/esm2_t6_8M_UR50D'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "eval_tokenized = tokenizer(df_eval['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "test_tokenized = tokenizer(df_test['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "eval_dataset = datasets.Dataset.from_dict(eval_tokenized).add_column('labels', df_eval['label'].tolist())\n",
    "test_dataset = datasets.Dataset.from_dict(test_tokenized).add_column('labels', df_test['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9.** There's a warning about uninitialized weights when loading the ESM-2 model using `AutoModelForSequenceClassification` below. Describe the part of the network that has the uninitialized weights. How does the new part connect to the rest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EsmForSequenceClassification(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (classifier): EsmClassificationHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=320, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=df_data['label'].nunique())\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Uninitialized weights:** the **new classification head** (`classifier.dense.*` and `classifier.out_proj.*`) is **MISSING** from the pretrained ESM2 checkpoint, so it’s randomly initialized.\n",
    "- **Connection:** pretrained **ESM encoder** outputs a **sequence embedding (dim=320)** → `classifier.dense (320→320)` → `classifier.out_proj (320→num_labels=6)` to produce class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f55efcc19b411c85ca7bfb45a05099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33ba64657384f2abee3810d0578bad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track accuracy and macro F1 score throughout the training\n",
    "# https://huggingface.co/docs/transformers/en/training#evaluate\n",
    "metric_accuracy = evaluate.load('accuracy')\n",
    "metric_f1 = evaluate.load('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': metric_accuracy.compute(predictions=predictions, references=labels)['accuracy'],\n",
    "        'f1_macro': metric_f1.compute(predictions=predictions, references=labels, average='macro')['f1'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** The fine-tuning may fail by running out of GPU memory. Look up `per_device_train_batch_size` and `per_device_eval_batch_size` in the [TrainingArguments docs](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/trainer#transformers.TrainingArguments). How would you adjust these parameters to use less GPU memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use **less GPU memory**, **decrease** `per_device_train_batch_size` and `per_device_eval_batch_size` (e.g., `8 → 4 → 2 → 1`).\n",
    "- If you still want a similar **effective batch size**, keep batches small and **increase** `gradient_accumulation_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /home/course/bc_deep_learning_in_biology/.venv/bin/python\n",
      "accelerate: 1.12.0\n",
      "transformers: 4.48.3\n",
      "torch: 2.10.0+cu128\n",
      "mps available: False\n"
     ]
    }
   ],
   "source": [
    "import sys, accelerate, transformers, torch\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"accelerate:\", accelerate.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"mps available:\", torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22008' max='22008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22008/22008 07:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.606700</td>\n",
       "      <td>0.741129</td>\n",
       "      <td>0.863125</td>\n",
       "      <td>0.618610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.454600</td>\n",
       "      <td>0.736177</td>\n",
       "      <td>0.870880</td>\n",
       "      <td>0.638570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.627858</td>\n",
       "      <td>0.895308</td>\n",
       "      <td>0.760889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up fine-tuning\n",
    "trainer_args = transformers.TrainingArguments(\n",
    "    output_dir=f'{model_checkpoint}-subcellular_location',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    # Adjust if needed\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,                         # Named\n",
    "    args=trainer_args,                   # Named (was 'trainer_args' in your snippet)\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,          # Use 'processing_class' or 'tokenizer'\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "retrained = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.8143479824066162,\n",
       " 'test_accuracy': 0.8699146157709694,\n",
       " 'test_f1_macro': 0.7421056811121813,\n",
       " 'test_runtime': 11.5419,\n",
       " 'test_samples_per_second': 172.502,\n",
       " 'test_steps_per_second': 172.502}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use fine-tuned model to predict on held-out test data\n",
    "retrained_predict = trainer.predict(test_dataset=test_dataset)\n",
    "retrained_predict.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8699146157709694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 2, 4, ..., 2, 5, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert probabilities into discrete predictions by taking the max probability\n",
    "test_labels = np.argmax(retrained_predict.predictions, axis=-1)\n",
    "# Sanity-check by manualy calculating the accuracy\n",
    "print(sum(test_labels == test_dataset['labels']) / len(test_dataset))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** Adjust the code below to re-produce the cross-validation results as in Table 2 of the paper. Fine-tune+test separately on every fold, and gather the results in `predicted_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 4} 1 0 6668 2684 2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20004' max='20004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20004/20004 06:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.669200</td>\n",
       "      <td>0.590921</td>\n",
       "      <td>0.887481</td>\n",
       "      <td>0.629164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.541300</td>\n",
       "      <td>0.605567</td>\n",
       "      <td>0.891952</td>\n",
       "      <td>0.690194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286900</td>\n",
       "      <td>0.591977</td>\n",
       "      <td>0.899404</td>\n",
       "      <td>0.723200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 3, 4} 2 1 7124 2098 2684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21372' max='21372' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21372/21372 06:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.573000</td>\n",
       "      <td>0.795080</td>\n",
       "      <td>0.868446</td>\n",
       "      <td>0.627092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.706297</td>\n",
       "      <td>0.884175</td>\n",
       "      <td>0.727017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.662465</td>\n",
       "      <td>0.885129</td>\n",
       "      <td>0.743165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 4} 3 2 7229 2579 2098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21687' max='21687' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21687/21687 07:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.990078</td>\n",
       "      <td>0.799147</td>\n",
       "      <td>0.589099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.387800</td>\n",
       "      <td>0.699573</td>\n",
       "      <td>0.873982</td>\n",
       "      <td>0.642722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.305100</td>\n",
       "      <td>0.745617</td>\n",
       "      <td>0.870105</td>\n",
       "      <td>0.698404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2} 4 3 7336 1991 2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22008' max='22008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22008/22008 07:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.676400</td>\n",
       "      <td>0.855345</td>\n",
       "      <td>0.844299</td>\n",
       "      <td>0.630254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470700</td>\n",
       "      <td>0.869421</td>\n",
       "      <td>0.863887</td>\n",
       "      <td>0.708826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>0.872674</td>\n",
       "      <td>0.857358</td>\n",
       "      <td>0.722181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3} 0 4 7361 2554 1991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22083' max='22083' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22083/22083 07:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.616900</td>\n",
       "      <td>0.884886</td>\n",
       "      <td>0.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>0.819566</td>\n",
       "      <td>0.846515</td>\n",
       "      <td>0.660137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.650250</td>\n",
       "      <td>0.895850</td>\n",
       "      <td>0.714506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fold_id = set(df_data.fold_id)\n",
    "predicted_labels_all = [None] * len(df_data)\n",
    "\n",
    "for test_id in sorted(fold_id):\n",
    "    eval_id = (test_id + 1) % 5\n",
    "    train_id = fold_id - {eval_id, test_id}\n",
    "\n",
    "    df_train = df_data.query('fold_id in @train_id')\n",
    "    df_eval  = df_data.query('fold_id == @eval_id')\n",
    "    df_test  = df_data.query('fold_id == @test_id')\n",
    "    print(train_id, eval_id, test_id, len(df_train), len(df_eval), len(df_test))\n",
    "\n",
    "    # --- Tokenize this fold's splits ---\n",
    "    train_tokenized = tokenizer(df_train['sequence'].tolist(), truncation=True, max_length=1024)\n",
    "    eval_tokenized  = tokenizer(df_eval['sequence'].tolist(),  truncation=True, max_length=1024)\n",
    "    test_tokenized  = tokenizer(df_test['sequence'].tolist(),  truncation=True, max_length=1024)\n",
    "\n",
    "    train_dataset = datasets.Dataset.from_dict(train_tokenized).add_column('labels', df_train['label'].tolist())\n",
    "    eval_dataset  = datasets.Dataset.from_dict(eval_tokenized).add_column('labels',  df_eval['label'].tolist())\n",
    "    test_dataset  = datasets.Dataset.from_dict(test_tokenized).add_column('labels',  df_test['label'].tolist())\n",
    "\n",
    "    # --- Fresh model for each fold ---\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=df_data['label'].nunique()\n",
    "    )\n",
    "\n",
    "    # --- Minimal TrainingArguments matching your working setup ---\n",
    "    trainer_args = transformers.TrainingArguments(\n",
    "        output_dir=f'{model_checkpoint}-fold{test_id}',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "    )\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        args=trainer_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Predict and store in correct positions ---\n",
    "    retrained_predict = trainer.predict(test_dataset=test_dataset)\n",
    "    fold_preds = list(np.argmax(retrained_predict.predictions, axis=-1))\n",
    "\n",
    "    for idx, pred in zip(df_test.index, fold_preds):\n",
    "        predicted_labels_all[df_data.index.get_loc(idx)] = pred\n",
    "\n",
    "predicted_labels = predicted_labels_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** Discuss differences between the methodology and the resulting performance of the approach taken in the paper, and the reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall</th>\n",
       "      <th>Archaea</th>\n",
       "      <th>Gram pos</th>\n",
       "      <th>Gram neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>11906</td>\n",
       "      <td>283</td>\n",
       "      <td>3206</td>\n",
       "      <td>8417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_macro</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Overall Archaea Gram pos Gram neg\n",
       "size       11906     283     3206     8417\n",
       "accuracy    0.89    0.85     0.91     0.88\n",
       "f1_macro    0.71    0.46     0.51     0.69"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show table with performance metrics split by organism to match Table 2\n",
    "def calculate_stats_(df):\n",
    "    accuracy = metric_accuracy.compute(predictions=df.predicted_labels.values, references=df.label.values)['accuracy']\n",
    "    f1_macro = metric_f1.compute(predictions=df.predicted_labels.values, references=df.label.values, average='macro')['f1']\n",
    "    return pd.Series({\n",
    "        'size': '{:d}'.format(len(df)),\n",
    "        'accuracy': '{:.2f}'.format(accuracy),\n",
    "        'f1_macro': '{:.2f}'.format(f1_macro),\n",
    "    })\n",
    "\n",
    "df_data['predicted_labels'] = predicted_labels\n",
    "pd.concat([\n",
    "    calculate_stats_(df_data),\n",
    "    calculate_stats_(df_data.query('organism_group == \"archaea\"')),\n",
    "    calculate_stats_(df_data.query('organism_group == \"positive\"')),\n",
    "    calculate_stats_(df_data.query('organism_group == \"negative\"')),\n",
    "], axis=1).set_axis(['Overall', 'Archaea' , 'Gram pos', 'Gram neg'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12 — Methodology & Performance: Paper vs. Reproduction\n",
    "\n",
    "### Methodology Differences\n",
    "\n",
    "| Aspect | Paper (DeepLocPro) | Reproduction |\n",
    "|---|---|---|\n",
    "| **ESM-2 backbone** | 650M parameters | 8M parameters |\n",
    "| **Pooling** | Learned attention pooling (per-residue weights) | Simple CLS-token / mean pooling |\n",
    "| **CV scheme** | Nested 5-fold: **20 models** trained (4 inner models × 5 outer folds), test predictions **averaged over 4 inner models per fold** | Simplified 5-fold: **5 models** (1 per outer fold, single fixed val fold = `(test_id + 1) % 5`), no ensemble averaging |\n",
    "| **Training length** | Up to **60 epochs** + dynamic LR reduction (×0.1 after 5 stagnant epochs) | **3 epochs**, default LR schedule |\n",
    "| **Batch size** | 8–64 (hyperparameter searched) | **1** (memory constraint) |\n",
    "| **Hyperparameter tuning** | Grid search over LR, batch size, dropout per fold | None |\n",
    "| **Regularization** | Dropout in classification head | None explicitly |\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Differences\n",
    "\n",
    "| Metric | Paper | Reproduction |\n",
    "|---|---|---|\n",
    "| **Overall Accuracy** | **0.92 ± 0.01** | ~0.87–0.90 |\n",
    "| **Macro F1** | **0.80 ± 0.02** | ~0.70–0.76 |\n",
    "\n",
    "Despite the gap, **qualitative patterns are preserved**: archaea remain the hardest group to classify, and cytoplasmic proteins remain the easiest — suggesting the fundamental biological signal is captured even by the smaller model.\n",
    "\n",
    "---\n",
    "\n",
    "### Why the Gap?\n",
    "\n",
    "The performance difference is attributable to several compounding factors:\n",
    "\n",
    "1. **Smaller backbone** — the 8M vs. 650M ESM-2 model encodes far less evolutionary and biochemical information\n",
    "2. **No attention pooling** — the paper's residue-level attention mechanism can up-weight informative regions (e.g. signal peptides); the reproduction cannot\n",
    "3. **Undertraining** — 3 epochs is likely insufficient relative to 60 epochs with adaptive LR decay\n",
    "4. **No ensembling** — averaging over 4 inner-loop models substantially reduces variance in the paper; the reproduction uses a single model per fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
